{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 7. Модель Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустить seq2seq, seq2seq с внимаием и трансформер для перевода русских слов + описать наблюдения по качеству"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Не самый лучший вариант, но наблюдение показало, что чем больше эпох и батчей, тем качество лучше. Далее практически, как в методичке, хотя пришлось порешать некоторые технические вопросы из-за конфликтов в версиях библиотек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, LSTM, Dense\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'E:/GB/NLP/data/rus-eng/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/125 [==============================] - 73s 587ms/step - loss: 1.1162 - accuracy: 0.7744 - val_loss: 0.9039 - val_accuracy: 0.7559\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 76s 605ms/step - loss: 0.7246 - accuracy: 0.8070 - val_loss: 0.7639 - val_accuracy: 0.7993\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 78s 623ms/step - loss: 0.6181 - accuracy: 0.8352 - val_loss: 0.6691 - val_accuracy: 0.8182\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 74s 589ms/step - loss: 0.5515 - accuracy: 0.8472 - val_loss: 0.6166 - val_accuracy: 0.8246\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 66s 525ms/step - loss: 0.5149 - accuracy: 0.8541 - val_loss: 0.5895 - val_accuracy: 0.8295\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 42s 337ms/step - loss: 0.4893 - accuracy: 0.8595 - val_loss: 0.5617 - val_accuracy: 0.8374\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 46s 369ms/step - loss: 0.4688 - accuracy: 0.8644 - val_loss: 0.5425 - val_accuracy: 0.8421\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.4511 - accuracy: 0.8687 - val_loss: 0.5283 - val_accuracy: 0.8464\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.4362 - accuracy: 0.8728 - val_loss: 0.5169 - val_accuracy: 0.8488\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 49s 395ms/step - loss: 0.4219 - accuracy: 0.8768 - val_loss: 0.5003 - val_accuracy: 0.8549\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 43s 344ms/step - loss: 0.4094 - accuracy: 0.8806 - val_loss: 0.4957 - val_accuracy: 0.8559\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 40s 320ms/step - loss: 0.3988 - accuracy: 0.8834 - val_loss: 0.4857 - val_accuracy: 0.8583\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.3868 - accuracy: 0.8870 - val_loss: 0.4789 - val_accuracy: 0.8614\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.3795 - accuracy: 0.8892 - val_loss: 0.4689 - val_accuracy: 0.8645\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.3669 - accuracy: 0.8929 - val_loss: 0.4656 - val_accuracy: 0.8663\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.3570 - accuracy: 0.8957 - val_loss: 0.4633 - val_accuracy: 0.8679\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.3477 - accuracy: 0.8987 - val_loss: 0.4526 - val_accuracy: 0.8699\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.3376 - accuracy: 0.9014 - val_loss: 0.4520 - val_accuracy: 0.8707\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.3300 - accuracy: 0.9035 - val_loss: 0.4470 - val_accuracy: 0.8725\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.3207 - accuracy: 0.9062 - val_loss: 0.4452 - val_accuracy: 0.8726\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 36s 292ms/step - loss: 0.3125 - accuracy: 0.9087 - val_loss: 0.4432 - val_accuracy: 0.8737\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.3039 - accuracy: 0.9110 - val_loss: 0.4432 - val_accuracy: 0.8745\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.2953 - accuracy: 0.9134 - val_loss: 0.4392 - val_accuracy: 0.8765\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 35s 276ms/step - loss: 0.2879 - accuracy: 0.9152 - val_loss: 0.4404 - val_accuracy: 0.8756\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.2800 - accuracy: 0.9178 - val_loss: 0.4386 - val_accuracy: 0.8770\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 33s 265ms/step - loss: 0.2720 - accuracy: 0.9201 - val_loss: 0.4363 - val_accuracy: 0.8770\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 32s 258ms/step - loss: 0.2650 - accuracy: 0.9216 - val_loss: 0.4335 - val_accuracy: 0.8788\n",
      "Epoch 28/30\n",
      "125/125 [==============================] - 33s 264ms/step - loss: 0.2577 - accuracy: 0.9240 - val_loss: 0.4397 - val_accuracy: 0.8782\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.2496 - accuracy: 0.9263 - val_loss: 0.4387 - val_accuracy: 0.8786\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.2432 - accuracy: 0.9283 - val_loss: 0.4332 - val_accuracy: 0.8806\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Уходите!\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Уходите!\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Уходите!\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здраствейтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здраствейтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здраствейтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здраствейтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здраствейтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Беги!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Беги!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Беги!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Беги!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Кто толо?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Карто!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Карто!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Карто!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Карто!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Карто!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Карто!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Пожит!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Пожит!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Пристом!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Пристом!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Пристом!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Перестань!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Перестань!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Перестань!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Do it.\n",
      "Decoded sentence: Поделжите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Продолжайте.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Продолжайте.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Притеть!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Притеть!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Притеть!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Притеть!\n",
      "\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: Потерей.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побегаю.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побегаю.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побегаю.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побегаю.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу собку.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу собку.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу собку.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я потарал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я потарал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я потарал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я подеждал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я подеждал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я подеждал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я подеждал.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Он ото!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Постаньтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Постаньтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Постаньтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Shoot!\n",
      "Decoded sentence: Не пробей!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнись.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнись.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнись.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнись.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнись.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнись.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Убирайтесь!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За боретесь!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За боретесь!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За боретесь!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За боретесь!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За боретесь!\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Поичите это.\n",
      "\n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: Поши меня.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замоть!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замоть!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замоть!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замоть!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Пономи.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Пономи.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Пономи.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди ейть.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Понял!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Поеня сойда!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Поеня сойда!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Поеня сойда!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Поеня сойда!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq с вниманием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, \n",
    "target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 1 Loss 0.1067\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 2 Loss 0.0304\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 3 Loss 0.0243\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 4 Loss 0.0212\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 5 Loss 0.0213\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 6 Loss 0.0178\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 7 Loss 0.0173\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 8 Loss 0.0174\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 9 Loss 0.0167\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 10 Loss 0.0151\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 11 Loss 0.0144\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 12 Loss 0.0141\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 13 Loss 0.0133\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 14 Loss 0.0123\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 15 Loss 0.0118\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 16 Loss 0.0114\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 17 Loss 0.0131\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 18 Loss 0.0123\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 19 Loss 0.0110\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 20 Loss 0.0102\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 21 Loss 0.0094\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 22 Loss 0.0114\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 23 Loss 0.0093\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 24 Loss 0.0090\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 25 Loss 0.0085\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 26 Loss 0.0084\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 27 Loss 0.0074\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 28 Loss 0.0079\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 29 Loss 0.0106\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 30 Loss 0.0107\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        if not batch % 25: \n",
    "            print(f'Step {batch}')\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> good morning <end>\n",
      "Predicted translation: . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f', 'char']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ticker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c62fe7b3b252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pylab'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'good morning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-61331b77bd12>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted translation: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mplot_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-61331b77bd12>\u001b[0m in \u001b[0;36mplot_attention\u001b[1;34m(attention, sentence, predicted_sentence)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpredicted_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ticker' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4760 Accuracy 0.0195\n",
      "Epoch 1 Batch 50 Loss 1.6916 Accuracy 0.1608\n",
      "Epoch 1 Batch 100 Loss 1.3562 Accuracy 0.2243\n",
      "Epoch 1 Loss 1.2238 Accuracy 0.2592\n",
      "Epoch 2 Batch 0 Loss 0.4146 Accuracy 0.4531\n",
      "Epoch 2 Batch 50 Loss 0.2847 Accuracy 0.4642\n",
      "Epoch 2 Batch 100 Loss 0.2188 Accuracy 0.4759\n",
      "Epoch 2 Loss 0.2061 Accuracy 0.4784\n",
      "Epoch 3 Batch 0 Loss 0.1120 Accuracy 0.4961\n",
      "Epoch 3 Batch 50 Loss 0.1251 Accuracy 0.4908\n",
      "Epoch 3 Batch 100 Loss 0.1170 Accuracy 0.4921\n",
      "Epoch 3 Loss 0.1157 Accuracy 0.4918\n",
      "Epoch 4 Batch 0 Loss 0.1014 Accuracy 0.4961\n",
      "Epoch 4 Batch 50 Loss 0.0957 Accuracy 0.4926\n",
      "Epoch 4 Batch 100 Loss 0.0939 Accuracy 0.4929\n",
      "Epoch 4 Loss 0.0957 Accuracy 0.4926\n",
      "Epoch 5 Batch 0 Loss 0.0481 Accuracy 0.5000\n",
      "Epoch 5 Batch 50 Loss 0.0867 Accuracy 0.4920\n",
      "Epoch 5 Batch 100 Loss 0.0871 Accuracy 0.4925\n",
      "Epoch 5 Loss 0.0878 Accuracy 0.4926\n",
      "Epoch 6 Batch 0 Loss 0.0264 Accuracy 0.5000\n",
      "Epoch 6 Batch 50 Loss 0.0840 Accuracy 0.4931\n",
      "Epoch 6 Batch 100 Loss 0.0788 Accuracy 0.4934\n",
      "Epoch 6 Loss 0.0790 Accuracy 0.4939\n",
      "Epoch 7 Batch 0 Loss 0.1973 Accuracy 0.4844\n",
      "Epoch 7 Batch 50 Loss 0.0726 Accuracy 0.4958\n",
      "Epoch 7 Batch 100 Loss 0.0727 Accuracy 0.4950\n",
      "Epoch 7 Loss 0.0706 Accuracy 0.4953\n",
      "Epoch 8 Batch 0 Loss 0.1514 Accuracy 0.4883\n",
      "Epoch 8 Batch 50 Loss 0.0631 Accuracy 0.4951\n",
      "Epoch 8 Batch 100 Loss 0.0643 Accuracy 0.4958\n",
      "Epoch 8 Loss 0.0646 Accuracy 0.4960\n",
      "Epoch 9 Batch 0 Loss 0.0828 Accuracy 0.4883\n",
      "Epoch 9 Batch 50 Loss 0.0618 Accuracy 0.4964\n",
      "Epoch 9 Batch 100 Loss 0.0620 Accuracy 0.4959\n",
      "Epoch 9 Loss 0.0633 Accuracy 0.4957\n",
      "Epoch 10 Batch 0 Loss 0.0873 Accuracy 0.4844\n",
      "Epoch 10 Batch 50 Loss 0.0628 Accuracy 0.4964\n",
      "Epoch 10 Batch 100 Loss 0.0660 Accuracy 0.4959\n",
      "Epoch 10 Loss 0.0612 Accuracy 0.4961\n",
      "Epoch 11 Batch 0 Loss 0.1814 Accuracy 0.4922\n",
      "Epoch 11 Batch 50 Loss 0.0561 Accuracy 0.4973\n",
      "Epoch 11 Batch 100 Loss 0.0548 Accuracy 0.4963\n",
      "Epoch 11 Loss 0.0524 Accuracy 0.4966\n",
      "Epoch 12 Batch 0 Loss 0.0168 Accuracy 0.5039\n",
      "Epoch 12 Batch 50 Loss 0.0598 Accuracy 0.4963\n",
      "Epoch 12 Batch 100 Loss 0.0546 Accuracy 0.4966\n",
      "Epoch 12 Loss 0.0552 Accuracy 0.4967\n",
      "Epoch 13 Batch 0 Loss 0.0615 Accuracy 0.4922\n",
      "Epoch 13 Batch 50 Loss 0.0455 Accuracy 0.4969\n",
      "Epoch 13 Batch 100 Loss 0.0493 Accuracy 0.4972\n",
      "Epoch 13 Loss 0.0498 Accuracy 0.4975\n",
      "Epoch 14 Batch 0 Loss 0.1152 Accuracy 0.4844\n",
      "Epoch 14 Batch 50 Loss 0.0481 Accuracy 0.4965\n",
      "Epoch 14 Batch 100 Loss 0.0500 Accuracy 0.4970\n",
      "Epoch 14 Loss 0.0502 Accuracy 0.4972\n",
      "Epoch 15 Batch 0 Loss 0.0257 Accuracy 0.4922\n",
      "Epoch 15 Batch 50 Loss 0.0425 Accuracy 0.4979\n",
      "Epoch 15 Batch 100 Loss 0.0483 Accuracy 0.4975\n",
      "Epoch 15 Loss 0.0511 Accuracy 0.4975\n",
      "Epoch 16 Batch 0 Loss 0.0158 Accuracy 0.5000\n",
      "Epoch 16 Batch 50 Loss 0.0430 Accuracy 0.4983\n",
      "Epoch 16 Batch 100 Loss 0.0479 Accuracy 0.4978\n",
      "Epoch 16 Loss 0.0504 Accuracy 0.4972\n",
      "Epoch 17 Batch 0 Loss 0.0336 Accuracy 0.4961\n",
      "Epoch 17 Batch 50 Loss 0.0427 Accuracy 0.4992\n",
      "Epoch 17 Batch 100 Loss 0.0447 Accuracy 0.4981\n",
      "Epoch 17 Loss 0.0493 Accuracy 0.4979\n",
      "Epoch 18 Batch 0 Loss 0.0491 Accuracy 0.4961\n",
      "Epoch 18 Batch 50 Loss 0.0498 Accuracy 0.4975\n",
      "Epoch 18 Batch 100 Loss 0.0504 Accuracy 0.4977\n",
      "Epoch 18 Loss 0.0514 Accuracy 0.4976\n",
      "Epoch 19 Batch 0 Loss 0.2862 Accuracy 0.4844\n",
      "Epoch 19 Batch 50 Loss 0.0475 Accuracy 0.4997\n",
      "Epoch 19 Batch 100 Loss 0.0469 Accuracy 0.4986\n",
      "Epoch 19 Loss 0.0494 Accuracy 0.4978\n",
      "Epoch 20 Batch 0 Loss 0.0164 Accuracy 0.5000\n",
      "Epoch 20 Batch 50 Loss 0.0399 Accuracy 0.4983\n",
      "Epoch 20 Batch 100 Loss 0.0438 Accuracy 0.4986\n",
      "Epoch 20 Loss 0.0451 Accuracy 0.4983\n",
      "Epoch 21 Batch 0 Loss 0.0283 Accuracy 0.5000\n",
      "Epoch 21 Batch 50 Loss 0.0447 Accuracy 0.4986\n",
      "Epoch 21 Batch 100 Loss 0.0472 Accuracy 0.4983\n",
      "Epoch 21 Loss 0.0461 Accuracy 0.4982\n",
      "Epoch 22 Batch 0 Loss 0.0160 Accuracy 0.5000\n",
      "Epoch 22 Batch 50 Loss 0.0509 Accuracy 0.4973\n",
      "Epoch 22 Batch 100 Loss 0.0478 Accuracy 0.4980\n",
      "Epoch 22 Loss 0.0485 Accuracy 0.4979\n",
      "Epoch 23 Batch 0 Loss 0.0216 Accuracy 0.5039\n",
      "Epoch 23 Batch 50 Loss 0.0394 Accuracy 0.4989\n",
      "Epoch 23 Batch 100 Loss 0.0432 Accuracy 0.4985\n",
      "Epoch 23 Loss 0.0458 Accuracy 0.4980\n",
      "Epoch 24 Batch 0 Loss 0.0313 Accuracy 0.4961\n",
      "Epoch 24 Batch 50 Loss 0.0390 Accuracy 0.4990\n",
      "Epoch 24 Batch 100 Loss 0.0453 Accuracy 0.4988\n",
      "Epoch 24 Loss 0.0448 Accuracy 0.4986\n",
      "Epoch 25 Batch 0 Loss 0.0365 Accuracy 0.5078\n",
      "Epoch 25 Batch 50 Loss 0.0475 Accuracy 0.4982\n",
      "Epoch 25 Batch 100 Loss 0.0445 Accuracy 0.4980\n",
      "Epoch 25 Loss 0.0449 Accuracy 0.4981\n",
      "Epoch 26 Batch 0 Loss 0.0852 Accuracy 0.4922\n",
      "Epoch 26 Batch 50 Loss 0.0513 Accuracy 0.4970\n",
      "Epoch 26 Batch 100 Loss 0.0487 Accuracy 0.4980\n",
      "Epoch 26 Loss 0.0473 Accuracy 0.4979\n",
      "Epoch 27 Batch 0 Loss 0.1065 Accuracy 0.4961\n",
      "Epoch 27 Batch 50 Loss 0.0388 Accuracy 0.4991\n",
      "Epoch 27 Batch 100 Loss 0.0460 Accuracy 0.4982\n",
      "Epoch 27 Loss 0.0468 Accuracy 0.4986\n",
      "Epoch 28 Batch 0 Loss 0.0937 Accuracy 0.5039\n",
      "Epoch 28 Batch 50 Loss 0.0419 Accuracy 0.4982\n",
      "Epoch 28 Batch 100 Loss 0.0451 Accuracy 0.4985\n",
      "Epoch 28 Loss 0.0479 Accuracy 0.4982\n",
      "Epoch 29 Batch 0 Loss 0.0237 Accuracy 0.4961\n",
      "Epoch 29 Batch 50 Loss 0.0445 Accuracy 0.4975\n",
      "Epoch 29 Batch 100 Loss 0.0444 Accuracy 0.4983\n",
      "Epoch 29 Loss 0.0455 Accuracy 0.4986\n",
      "Epoch 30 Batch 0 Loss 0.0262 Accuracy 0.5078\n",
      "Epoch 30 Batch 50 Loss 0.0532 Accuracy 0.4982\n",
      "Epoch 30 Batch 100 Loss 0.0490 Accuracy 0.4977\n",
      "Epoch 30 Loss 0.0491 Accuracy 0.4980\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(30):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', '.']\n"
     ]
    }
   ],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(\"good morning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: how are you?\n",
      "Predicted translation: ['<start>', '?']\n"
     ]
    }
   ],
   "source": [
    "translate(\"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
